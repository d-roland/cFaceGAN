## Abstract

We compare conditional generative adversarial networks (cGANs) with different numbers of conditions. We train progressively larger GANs in order to improve stability and variation and utilize the auxiliary classifier approach to conditioning. Qualitatively, our conditional GANs performed well regardless of the number of conditions and generated images accordingly. Quantitatively, our forty conditions GAN performed worse than models with fewer conditions as measured by Sliced Wasserstein Distance and Inception Score.

## 1. Introduction

Conditional generative adversarial networks which allow a trained generator to output samples that comply with certain conditions have been proposed since the advent of generative adversarial networks [5]. We examine the effect of changing the number of conditions on the training of these conditional GANs as well as the accuracy of output samples in terms of following their conditions. The number of conditions can theoretically affect the accuracy of output samples by limiting the number of samples which match a specific combination of conditions during training. We examine how number of samples changes as number of conditions increases and how that affects our GAN training and generator performance. We train our conditional GAN using labeled facial images so that the trained generator is able to produce face images corresponding to difference facial characteristics. The ability to generate images of a face given certain parameters has applications in avatar generation and gaming. For our method, we plan to train a conditional deep convolutional GAN (DCGAN) as implemented in [16]. To improve output image quality we utilize progressively deeper GANs as in [8].

## 2. Related Work
GANs have drawn lots of attention over recent years asthey excelled at (re-)creating realistic images in multiple contexts: face [18] or age [12] synthesis,  realistic photograph [15] [10] or human pose generation [4] for example. Conditional GANs also witnessed great success as their performances improved significantly over the past years. Latest most visible applications included image to image translation with models such as StarGAN [2] or DualGAN [20], but also text to image synthesis [21] with models such as StackGAN [21] or DCGAN [16]. As performance grew, new metrics were needed for both monitoring and benchmarking: the Frechet Inception Distance (FID) [7] and Inception Score [17] are notably used by most recent models. More recently, the FID got fine tuned to produce the Frechet Joint Distance (FJD) [3]. Very recently, we noticed promising results from the MIT versus the computationally intensive drawback of cGANs [11], which prevents them from being deployed on edge devices like mobile phones, tablets or VR headsets with insufficient hardware resources, memory or power. This could increase their usage in mobile gaming context, for instance for avatar generation or profile picture customization.

## 3. Problem Statement
### 3.1 Creating a conditional generator
We are creating a generator neural network that takes input vector c_in of binary conditions and outputs a random face image that matches those binary conditions.  The generator function also takes in latent input z which is usually random and ensures there is variability in the output image for a given c_in. The output image is an RBG image with size 128×128 pixels. 
### 3.2 The training process
The generator is trained as part of training a conditional generative adversarial network. The inputs to the training process are real facial images along with their associated binary labels. We describe the specifics of how the training process works in section 4. The output of the training process are trained generator and discriminator networks. The trained generator is then used to generate images satisfying input conditions.

## 4. Technical Approach
### 4.1. Progressive GAN architecture and training process
We utilize the network architecture described in [8]. This convolutional architecture grows progressively deeper during training. During training, both the image resolution and network depth increases. The generator and discriminator network depths grow together symmetrically. The final generator and discriminator networks are shown in Figures 1 and 2. The training starts at a level of detail which corresponds to 4×4 images. For the generator, only the firstblock of two convolutional layers are used. Then there is a final Conv1×1layer which outputs a 4×4 image. As the level of detail changes by one, the output image’s height and width grows by a factor of two, and for the generator network, we add an upsample and two convolutional layers prior to the final Conv1×1. For the discriminator network, when training with 4×4 images, we only have the final block shown in Figure 2. We have a Conv 1×1 layer right before the Minibatch Standard Deviation layer. We refer the reader to the original paper [8] for details on the Minibatch Standard Deviation layer. As the level of detail changes by one, the dimensions of the input image to the discriminator doubles, and we add the next block of two Conv 3×3 layers and Downsample layers. During the entire training, the level of detail progressively changes a total of five times where the images increase from 4×4 to 128×128 pixels. A total of over 6 million images are fed through the training process.
### 4.2. Auxiliary classifer approach to conditioning
Our  baseline  is  a  GAN  which  does  not  take  any  conditions for c_in. We increase the number of conditions to compare against our baseline. The approach we  take to condition our GAN is using the auxiliary classifier approach as described in [14]. The generator network takes in a condition variable c_in as mentioned earlier. The difference between the auxiliary classifier approach versus vanilla conditioning is that the condition vector c_in is not an input to the discriminator. Instead, the discriminator is tasked with labeling the input image with a correct c. In addition to the traditional loss functions minimized for the generator and discriminator, the auxiliary classifer conditioning adds a label penalty to both of their respective loss functions. We use the binary cross entropy loss function for each component of c_in where y_i is the true label and ŷ_i is the sigmoid output from the discriminator for label component i.

